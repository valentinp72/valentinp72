<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Valentin Pelloin</title>
    <link>https://vpelloin.eu/posts/</link>
    <description>Recent content in Posts on Valentin Pelloin</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 14 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://vpelloin.eu/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>:bullettrain_side: Interrail 2023: Denmark, Sweden, Germany, The Netherlands, Belgium</title>
      <link>https://vpelloin.eu/posts/interrail-2023/</link>
      <pubDate>Mon, 14 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://vpelloin.eu/posts/interrail-2023/</guid>
      <description>&lt;p&gt;This summer, I did my second Interrail trip. With an &lt;a href=&#34;https://www.interrail.eu/en&#34;&gt;Interrail&lt;/a&gt; pass, you can travel with many different trains throughout Europe with little to no additional cost. In this post, I will share some pictures I made during this trip through 5 different countries, as well as share the route and trains I have taken.&lt;/p&gt;
&lt;!-- My previous Interrail trip was described in [this post](/posts/interrail-2021). --&gt;</description>
    </item>
    
    <item>
      <title>:books: Notes: How does sequence generation works in Fairseq/Espresso?</title>
      <link>https://vpelloin.eu/posts/fairseq-sequence-generation/</link>
      <pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://vpelloin.eu/posts/fairseq-sequence-generation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/fairseq&#34;&gt;Fairseq&lt;/a&gt; is a &amp;ldquo;sequence sequence modeling toolkit written in &lt;a href=&#34;http://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks&amp;rdquo;. I will try to explain how fairesq generates sequences with a Language Model while using a beam search algorithm.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>:books: Notes: Attention! Transformers!</title>
      <link>https://vpelloin.eu/posts/attention-transformers/</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://vpelloin.eu/posts/attention-transformers/</guid>
      <description>&lt;p&gt;Attention mechanisms, used in encoder-decoder reccurrent neural networks are able to align an input sequence to an output sequence.
The transformer, described in &amp;ldquo;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention Is All You Need&lt;/a&gt;&amp;rdquo; is a network architecture capable of sequence-to-sequence translations, without using recurrence, and thus, allowing parallelization.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
