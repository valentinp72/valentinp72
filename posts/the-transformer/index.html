<!DOCTYPE html>
<html><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Notes: The Transformer</title>
    <base href="https://valentinp72.fr/">
    <meta name="description" content="">
    <meta name="author" content='Valentin Pelloin'>

    <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    
    <link rel="stylesheet" href="/sass/researcher.min.css">

    
        <link rel="icon" type="image/ico" href="https://valentinp72.fr/favicon.ico">
    
</head>
<body><div class="container mt-5">
    <nav class="navbar navbar-expand-sm flex-column flex-sm-row p-0">
        <a class="navbar-brand mr-sm-auto" href="https://valentinp72.fr/">Valentin Pelloin</a>
        <div class="navbar-nav flex-row">
            
                
                
                    <a class="nav-item nav-link" href="/about">About</a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="/posts">Posts</a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="/contact">Contact</a>
                    
                
            
        </div>
    </nav>
</div>
<hr>
<div id="content">
<div class="container">
	<h1>Notes: The Transformer</h1>
	Sep. 7, 2020
	
	
	<hr/>
	<p>The transformer, described in &ldquo;<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>&rdquo; is a network architecture capable of sequence-to-sequence translations, without using recurrence, and thus, allowing parallelization.</p>
<h2 id="the-transformer">The Transformer</h2>
<p>In order to solve sequence transduction or sequence-to-sequence
problems (i.e. from $(x_1, x_2, &hellip;, x_n)$ to $(y_1, y_n, &hellip;, y_m)$, we can use an encoder followed by a decoder.</p>
<p>As classic recurrent (RNN) encoder-decoder networks, the Transformer is made of an encoder, connected to the decoder. In  &ldquo;<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>&rdquo;, 6 encoder layers are stacked together, followed by 6 other decoder layer.</p>
<p>An encoder layer is made of a self-attention component followed by a feed-forward NN.</p>
<blockquote>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values [&hellip;]. [1]</p>
</blockquote>
<h3 id="credits">Credits</h3>
<ol>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
</ol>
	</div>

        </div>
    <div id="footer">
        <hr>
        <div class="container text-center mb-2">
            <a href="https://github.com/ojroques/hugo-researcher"><small>Theme by Olivier Roques</small></a>
        </div>
    </div>

</body>
</html>
