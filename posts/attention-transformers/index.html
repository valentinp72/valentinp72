<!DOCTYPE html>
<html><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Notes: Attention! Transformers!</title>
    <base href="https://vpelloin.fr/">
    <meta name="description" content="">
    <meta name="author" content='Valentin Pelloin'>

    <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    
    <link rel="stylesheet" href="/sass/researcher.min.css">

    
        <link rel="icon" type="image/ico" href="https://vpelloin.fr/favicon.ico">
		<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
		<link rel="manifest" href="/site.webmanifest">
    

	


























<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [['$','$']],
      displayMath: [['$$','$$']],
      processEscapes: true
    },
    "HTML-CSS": { fonts: ["TeX"], linebreaks: { automatic: true, width: "container" } }
  });
</script>

<script type="text/javascript"
		   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js">
</script>

</head>
<body><div class="container mt-5">
    <nav class="navbar navbar-expand-sm flex-column flex-sm-row p-0">
        <a class="navbar-brand mr-sm-auto" href="https://vpelloin.fr/">Valentin Pelloin</a>
        <div class="navbar-nav flex-row">
            
                
                
                    <a class="nav-item nav-link" href="/about">About</a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="/posts">Posts</a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="/gallery">Gallery</a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="/contact">Contact</a>
                    
                
            
        </div>
    </nav>
</div>
<hr>
<div id="content">
<div class="container">
	<h1>Notes: Attention! Transformers!</h1>
	Sep. 7, 2020
	
	
	<hr/>
	<p>Attention mechanisms, used in encoder-decoder reccurrent neural networks are able to align an input sequence to an output sequence.
The transformer, described in &ldquo;<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>&rdquo; is a network architecture capable of sequence-to-sequence translations, without using recurrence, and thus, allowing parallelization.</p>
<h2 id="encoder-decoder-with-attention-mechanisms">Encoder-Decoder with attention mechanisms</h2>
<p>In order to solve sequence transduction or sequence-to-sequence problems (i.e. from $(x_1, x_2, &hellip;, x_n)$ to $(y_1, y_n, &hellip;, y_m)$, we can use an encoder followed by a decoder. In neural machine translation, standard encoder-decoder networks aims at &ldquo;encoding a source sentence into a fixed-length vector from which a decoder generates a translation&rdquo; [1]. According to [1], &ldquo;the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture&rdquo;. Compressing all the source sentence information into a single vector that is fixed in length is tough.</p>
<p>Attention mechanisms, first introduced by [1], doesn&rsquo;t want to reduce all the information in a single vector, but instead, it &ldquo;encodes the input sequence into a sequence of vectors, and chooses a subset of these vectors adaptively while decoding the translation&rdquo;.</p>
<p>During each encoding step (from $1$ to $n$), an input hidden state $h_j$ is generated by the Reccurrent Neural Network. A context vector $c_i$ is built using all those $j$ hidden states. In Bahdanau attention [1], the context vector is computed as a the weighted sum of the input hidden states $h_j$. Weight is computed using the softmax function, on the alignment model. The alignment model is given using a feedforward neural network that takes the input hidden state $h_j$ and the previous output hidden state $s_{i-1}$.</p>
<p>Some other similar attention mechanisms uses different alingment models.</p>
<p>In [4] was generalised the attention functions using a set of query, key and value that gives an output:</p>
<blockquote>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values [&hellip;]. [5]</p>
</blockquote>
<div>
  $$
  \text{attention function} = \text{Weighted sum of the values, weight computed with a compatibily function between query and key} \\
\Rightarrow \text{output} = \sum \text{values}\times\text{compatibily-function(query, key)}
  $$
</div>
<p>With this, we can write Bahdanau&rsquo;s attention as:</p>
<div>
  $$
  \begin{align}
\text{context} & = \sum h_j \times \text{NN-softmax}(s_{i-1}, h_j) \\
&\text{where:} \\
\text{keys} & = h_j  \\
\text{values} & = h_j  \\
\text{query} &= s_{i-1} \\
\text{output} &= \text{context}
\end{align}
  $$
</div>
<h3 id="global-vs-local-attention">Global vs Local Attention</h3>
<p>Global attention allow the attention to attend on all the source positions, whereas local attention allows attention on only a few source positions [2]. Attending to all source words is computationally expensive, hence limiting the alignment to a small context window.</p>
<p>In [3] where presented hard and soft attentions, which are similiar to local and global attention :</p>
<ul>
<li>Soft-attention : similar to global attention, where &ldquo;wheights are sloftly placed over all patches&rdquo;</li>
<li>Hard-attention : similar to local attention, but more complicated and not <a href="https://en.wikipedia.org/wiki/Differentiable_function">differentiable</a></li>
</ul>
<h3 id="self-attention">Self-attention</h3>
<p>Self-attention (or intra-attention), introduced in [4] also creates three vectors from the encoder input: the query, the key and the value.</p>
<div>
  $$
  \text{context}_t = \sum \text{value}_t \times \text{softmax}\left(\frac{\text{query}_t \cdot \text{key}_t^T}{\sqrt{d_{\text{key}_t}}}\right)
  $$
</div>
<p>Query, key and value (at time step $t$ are vectors extracted from the input embedding $t$: we compute them by multiplying the input $t$ with trained weights matrices $W^Q$, $W^K$ and $ W^V$. Self attention is attention between words within the same sentence, whereas Bahdanau&rsquo;s attention for exemple attend words between the source sequence and the output sequence (between the encoder and the decoder).</p>
<h5 id="whats-the-difference-between-this-self-attention-mechanism-and-the-standard-attention">What&rsquo;s the difference between this self-attention mechanism and the standard attention?</h5>
<ul>
<li>In &ldquo;standard&rdquo;-attention, the query vector is the decoder RNN hidden state: it doesn&rsquo;t come from the encoder network. Here, the query is just a transformation of the input $x_t$ using the weight matrice $W^Q$: data comes from the same network.</li>
<li>The compatibility function here is just a softmax, whereas in the standard attention is the softmax of an output given by a feedforward neural network.</li>
<li>Key and value vectors are here different vectors, in the standard attention both where the encoder hidden state $h_j$.</li>
</ul>
<p>The difference between intra and inter attention is described in [6]:</p>
<ul>
<li>Intra-attention : inside the encoder or decoder themselves</li>
<li>Inter-attention : &ldquo;standard&rdquo; attention, between the encoder and the decoder</li>
</ul>
<h5 id="how-are-the-weights-matrices-wq-wk-and-wv-learned">How are the weights matrices $W^Q$, $W^K$ and $W^V$ learned?</h5>
<p>Learned during training.</p>
<h3 id="mutli-head-attention">Mutli-head attention</h3>
<p>Usign multi-head attention, as described in [5], not a signle self-attention function is computed, but instead $h$ different self-attention &ldquo;heads&rdquo; are computed, and then concatenated together, and then projected again using an other weight matrice $W^O$. Each attention layer is computed in parallel.</p>
<h2 id="the-transformer">The Transformer</h2>
<p>As classic recurrent (RNN) encoder-decoder networks, the Transformer is made of an encoder, connected to the decoder. In  &ldquo;<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>&rdquo;, 6 encoder layers are stacked together, followed by 6 other decoder layer.</p>
<p>Three types of muli-head attentions are used in the transformer :</p>
<ul>
<li>Self-attention inside the encoder layers.</li>
<li>Self-attention inside the decoder layers.</li>
<li>Inter attentention / standard attention inside the decoder, with queries from the decoder and keys and values from the encoder.</li>
</ul>
<p>With this layout, an encoder layer is made of a self-attention component followed by a feed-forward NN. An decoder layer contains a self attention, a classical encoder-decoder inter attention and then an other feed-forward NN.</p>
<h3 id="references">References</h3>
<ol>
<li><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1502.03044">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></li>
<li><a href="https://www.aclweb.org/anthology/N16-1174.pdf">Hierarchical Attention Networks for Document Classification</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
<li><a href="https://arxiv.org/abs/1601.06733">Long Short-Term Memory-Networks for Machine Reading</a></li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">Attention? Attention!</a></li>
<li><a href="https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc">Attention and its Different Forms</a></li>
<li><a href="https://www.reddit.com/r/LanguageTechnology/comments/be6jfc/what_is_the_difference_between_self_attention_and/">[Reddit] What is the difference between self attention and attention</a></li>
</ol>
	</div>

        </div>
    <div id="footer">
        <hr>
        <div class="container text-center mb-2">
            <a href="https://github.com/ojroques/hugo-researcher"><small>Theme by Olivier Roques</small></a>
        </div>
    </div>


</body>
</html>
